{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142f49f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "RMSE: 0.0096\n",
      "MAE: 0.0309\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('archive/events.csv')\n",
    "\n",
    "# Keep only the relevant columns (visitorid, itemid, and event type)\n",
    "data = data[['visitorid', 'itemid', 'event']]\n",
    "\n",
    "# Convert the event column to binary ratings (1 for transaction, 0 otherwise)\n",
    "data['rating'] = data['event'].apply(lambda x: 1 if x == 'transaction' else 0)\n",
    "print(1)\n",
    "# Create a Surprise Reader object\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "\n",
    "# Load the data into a Surprise Dataset\n",
    "dataset = Dataset.load_from_df(data[['visitorid', 'itemid', 'rating']], reader)\n",
    "print(2)\n",
    "# Split the dataset into train and test sets for collaborative filtering\n",
    "trainset, testset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the collaborative filtering model (SVD is used here as an example)\n",
    "model_collab = SVD()\n",
    "model_collab.fit(trainset)\n",
    "print(3)\n",
    "# Function to get collaborative filtering recommendations for a given user\n",
    "def get_collab_recommendations(user_id, model, top_n=10):\n",
    "    items_to_predict = data['itemid'].unique()\n",
    "    user_items = [(user_id, item_id, 0) for item_id in items_to_predict]\n",
    "    predictions = model.test(user_items)\n",
    "    recommended_items = [(pred.iid, pred.est) for pred in predictions]\n",
    "    recommended_items.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [item[0] for item in recommended_items[:top_n]]\n",
    "\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error\n",
    "\n",
    "# Define thresholds\n",
    "positive_prediction_threshold = 0.5\n",
    "relevant_item_threshold = 1\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model_collab.test(testset)\n",
    "\n",
    "# Initialize variables for evaluation metrics\n",
    "true_positives, false_positives, true_negatives, false_negatives = 0, 0, 0, 0\n",
    "predicted_ratings = []\n",
    "actual_ratings = []\n",
    "for uid, iid, true_rating, est, _ in predictions:\n",
    "  predicted_rating = 1 if est > positive_prediction_threshold else 0\n",
    "  actual_rating = 1 if true_rating > relevant_item_threshold else 0\n",
    "  predicted_ratings.append(est)\n",
    "  actual_ratings.append(true_rating)\n",
    "  if predicted_rating == actual_rating:\n",
    "    if predicted_rating == 1:\n",
    "      true_positives += 1\n",
    "    else:\n",
    "      true_negatives += 1\n",
    "  else:\n",
    "    if predicted_rating == 1:\n",
    "      false_positives += 1\n",
    "    else:\n",
    "      false_negatives += 1\n",
    "\n",
    "# Calculate and print the evaluation metrics\n",
    "# recall = recall_score(actual_ratings, predicted_ratings)\n",
    "# precision = precision_score(actual_ratings, predicted_ratings)\n",
    "# f1 = f1_score(actual_ratings, predicted_ratings)\n",
    "# rmse = mean_squared_error(actual_ratings, predicted_ratings)**0.5\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = mean_squared_error(actual_ratings, predicted_ratings)\n",
    "mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"F1 score: {f1:.4f}\")\n",
    "# print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9742d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
